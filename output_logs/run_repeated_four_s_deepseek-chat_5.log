System Prompt:  
You are an HLS Optimization Agent tasked with optimizing a C application accelerated using High-Level Synthesis (HLS). 
You are part of an automated agentic flow, please reply following the instructed format.
Your goal is to find the best combination of function options that minimizes latency while keeping the total area as close as possible to a target value. 
At every iteration you have three options:
1. Synthesize a new configuration to evaluate its latency throughput and area.
If you select this option you should replay in the following format:
"synthesis: <function_name_1> <option_index>, <function_name_2> <option_index>, ..., <function_name_n> <option_index>" for each function in the application.
I will run the synthesis and provide you with the results.
2. See one or more function options.
If you select this option you should reply with the following format:
"inspect: <function_name_1> <option_index>, <function_name_2> <option_index>, ..., <function_name_n> <option_index>"
3. Run the a python script to solve an optimization problem using the google OR-Tools library.
If you select this option you should reply with the following format:
"python: '''<python_scipt_to_run>'''"
I will run the script and provide you with the results.
4. Run gprof on the applacation to profile the code.
If you select this option you should reply with the following format:
"profile"
I will run the gprof and provide you with the results.
5. Accept a solution and provide the final configuration.
If you select this option you should reply with the following format:
"solution: <function_name_1> <option_index>, <function_name_2> <option_index>, ..., <function_name_n> <option_index>"

Only reply with one of the five options following the format provided.

Prompt:  
This is the call graph for the application at hand:
{'odd_factorial': [], 'fibonacci': [], 'even_sum': [], 'compute6': ['odd_factorial', 'fibonacci', 'even_sum', 'even_sum'], 'main': ['compute6', 'printf']}

The application has the following functions with their respective options:
Option for odd_factorial -  area: 3696.9, latency: 2.0, throughput: 3.0
Option for odd_factorial -  area: 3696.9, latency: 2.0, throughput: 3.0
Option for odd_factorial -  area: 3583.9, latency: 2.0, throughput: 3.0
Option for odd_factorial -  area: 3583.9, latency: 2.0, throughput: 3.0
Option for odd_factorial -  area: 3583.9, latency: 2.0, throughput: 3.0
Option for fibonacci -  area: 1640.9, latency: 2.0, throughput: 3.0
Option for fibonacci -  area: 1640.9, latency: 2.0, throughput: 3.0
Option for fibonacci -  area: 1728.2, latency: 2.0, throughput: 3.0
Option for fibonacci -  area: 1728.2, latency: 2.0, throughput: 3.0
Option for fibonacci -  area: 1640.9, latency: 2.0, throughput: 3.0
Option for even_sum -  area: 1349.7, latency: 2.0, throughput: 3.0
Option for even_sum -  area: 1349.7, latency: 2.0, throughput: 3.0
Option for even_sum -  area: 1230.0, latency: 2.0, throughput: 3.0
Option for even_sum -  area: 1230.0, latency: 2.0, throughput: 3.0
Option for even_sum -  area: 1349.7, latency: 2.0, throughput: 3.0
Option for compute6 -  area: 8510.3, latency: 39.0, throughput: 41.0
Option for compute6 -  area: 8510.3, latency: 39.0, throughput: 41.0
Option for compute6 -  area: 8060.6, latency: 55.0, throughput: 59.0
Option for compute6 -  area: 8510.3, latency: 39.0, throughput: 41.0
Option for compute6 -  area: 8141.9, latency: 50.0, throughput: 54.0


Use the function names as provided, indexing the options starting from 0
The throughput is expressed in period, the lower the better.

The values reported for a function are obtained synthesizing the function in isolation, selecting the best latency from the child functions.
This means that the child functions are included in the function synthesis. Selecting a different option will affect the final outcome of the parent function.
Different options use different unrolling and pipelining factors. 

The goal is to minimize the latency while keeping the total area as close as possible to 7000.

Provide your first request:

error calling the LLM API
Error: Error code: 503 - {'error': {'message': 'Service is too busy. We advise users to temporarily switch to alternative LLM API service providers.', 'type': 'service_unavailable_error', 'param': None, 'code': 'invalid_request_error'}}
Traceback (most recent call last):
  File "/home/lc4976/repair_6/src/repair.py", line 364, in call_llm
    completion = cfg.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/_utils/_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 667, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/_base_client.py", line 1213, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/_base_client.py", line 902, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/_base_client.py", line 978, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/_base_client.py", line 1026, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/_base_client.py", line 978, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/_base_client.py", line 1026, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/lc4976/.local/lib/python3.11/site-packages/openai/_base_client.py", line 993, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service is too busy. We advise users to temporarily switch to alternative LLM API service providers.', 'type': 'service_unavailable_error', 'param': None, 'code': 'invalid_request_error'}}

